{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### zzzz"
      ],
      "metadata": {
        "id": "4PkfKuWmrg7Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import urllib.request\n",
        "import zipfile\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from tensorflow.keras.optimizers import SGD"
      ],
      "metadata": {
        "id": "3zeAJajdritU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download and extract the dataset (assuming it's a zip file)\n",
        "url = \"https://github.com/onesinus/datasets/raw/main/brain_tumor_detection.zip\"\n",
        "filename = \"brain_tumor_detection.zip\"\n",
        "dataset_dir = \"/content/dataset\"  # Directory to store extracted data\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "os.makedirs(dataset_dir, exist_ok=True)\n",
        "\n",
        "if not os.path.exists(filename):\n",
        "  print(\"Downloading dataset...\")\n",
        "  urllib.request.urlretrieve(url, filename)\n",
        "  print(\"Download complete.\")\n",
        "\n",
        "# Extract the downloaded zip file\n",
        "if os.path.exists(filename):\n",
        "  print(\"Extracting dataset...\")\n",
        "  with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
        "    zip_ref.extractall(dataset_dir)\n",
        "  os.remove(filename)  # Remove downloaded zip\n",
        "  print(\"Extraction complete.\")\n",
        "\n",
        "# Define label mapping (assuming class labels are encoded as strings)\n",
        "label_map = {'0': 0, '1': 1, '2': 2}  # Modify if class labels are encoded differently\n",
        "\n",
        "# Image dimensions (adjust if images have different sizes)\n",
        "img_width, img_height = 224, 224  # Example size, adjust based on your data\n",
        "\n",
        "# Data augmentation (optional, experiment to see if it improves performance)\n",
        "train_datagen = ImageDataGenerator(rescale=1./255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True)\n",
        "valid_datagen = ImageDataGenerator(rescale=1./255)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CSADOvx3txl9",
        "outputId": "63712a09-7c31-4e67-c965-5a745dda6659"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading dataset...\n",
            "Download complete.\n",
            "Extracting dataset...\n",
            "Extraction complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(data_dir, images_dir_name, labels_dir_name, label_map):\n",
        "  \"\"\"Loads images and labels from separate directories, handling multiple labels per file.\"\"\"\n",
        "  images = []\n",
        "  labels = []\n",
        "  for filename in os.listdir(os.path.join(data_dir, images_dir_name)):\n",
        "    if filename.endswith(\".jpg\"):\n",
        "      img_path = os.path.join(data_dir, images_dir_name, filename)\n",
        "      img = tf.keras.preprocessing.image.load_img(img_path, target_size=(img_width, img_height))\n",
        "      img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
        "      img_array = img_array / 255.0  # Normalize pixel values\n",
        "\n",
        "      label_filename = os.path.splitext(filename)[0] + \".txt\"  # Remove extension and add .txt\n",
        "      label_path = os.path.join(data_dir, labels_dir_name, label_filename)\n",
        "\n",
        "      with open(label_path, 'r') as f:\n",
        "        label_lines = f.readlines()  # Read all lines in the label file\n",
        "\n",
        "      # Assuming multiple labels per line (modify if different)\n",
        "      one_hot_labels = np.zeros(len(label_map))  # Initialize one-hot encoded labels\n",
        "      for line in label_lines:\n",
        "        label_str = line.strip()\n",
        "        label_values = label_str.split()\n",
        "        class_value = label_values[0]\n",
        "        # print(label_str)\n",
        "        if class_value in label_map:\n",
        "          label_index = label_map[class_value]\n",
        "          one_hot_labels[label_index] = 1.0  # Set corresponding label to 1 in one-hot vector\n",
        "\n",
        "      images.append(img_array)\n",
        "      labels.append(one_hot_labels)\n",
        "\n",
        "  return np.array(images), np.array(labels)\n",
        "\n",
        "import time # will be deleted later, just to save memory hehe\n",
        "\n",
        "# Load training and validation data (assuming separate directories)\n",
        "train_images, train_labels = load_data(dataset_dir, 'Brain Tumor Detection/train/images', 'Brain Tumor Detection/train/labels', label_map)\n",
        "time.sleep(3)\n",
        "validation_images, validation_labels = load_data(dataset_dir, 'Brain Tumor Detection/valid/images', 'Brain Tumor Detection/valid/labels', label_map)\n",
        "time.sleep(3)\n",
        "test_images, test_labels = load_data(dataset_dir, 'Brain Tumor Detection/test/images', 'Brain Tumor Detection/test/labels', label_map)\n",
        "\n",
        "# Create training and validation generators\n",
        "train_generator = train_datagen.flow(train_images, train_labels, batch_size=100)\n",
        "time.sleep(3)\n",
        "validation_generator = valid_datagen.flow(validation_images, validation_labels, batch_size=100)\n",
        "time.sleep(3)\n",
        "test_generator = valid_datagen.flow(test_images, test_labels, batch_size=100)"
      ],
      "metadata": {
        "id": "VpTAc8KUsAT3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print shapes for training data\n",
        "print(f\"Shape of X (training images): {train_images.shape}\")\n",
        "print(f\"Shape of Y (training labels): {train_labels.shape}\")\n",
        "\n",
        "# print(f\"Training Image [0]: {train_images[0]}\")\n",
        "print(f\"Training Label [0]: {train_labels[0]}\")\n",
        "\n",
        "for i in range(1,10):\n",
        "  print(f\"Training Label [{i}]: {train_labels[i]}\")\n",
        "\n",
        "# Print shapes for validation data\n",
        "print(f\"Shape of X (validation images): {validation_images.shape}\")\n",
        "print(f\"Shape of Y (validation labels): {validation_labels.shape}\")\n",
        "\n",
        "# Assuming you have loaded test data using the same approach\n",
        "print(f\"Shape of X (test images): {test_images.shape}\")\n",
        "print(f\"Shape of Y (test labels): {test_labels.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HUOZhCNkvBR0",
        "outputId": "5281fdde-0155-4397-c6c6-1542a22816fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X (training images): (6930, 224, 224, 3)\n",
            "Shape of Y (training labels): (6930, 3)\n",
            "Training Label [0]: [1. 1. 1.]\n",
            "Training Label [1]: [0. 1. 0.]\n",
            "Training Label [2]: [0. 1. 0.]\n",
            "Training Label [3]: [1. 1. 1.]\n",
            "Training Label [4]: [1. 1. 1.]\n",
            "Training Label [5]: [1. 1. 0.]\n",
            "Training Label [6]: [1. 1. 1.]\n",
            "Training Label [7]: [0. 1. 1.]\n",
            "Training Label [8]: [1. 1. 1.]\n",
            "Training Label [9]: [0. 1. 0.]\n",
            "Shape of X (validation images): (1980, 224, 224, 3)\n",
            "Shape of Y (validation labels): (1980, 3)\n",
            "Shape of X (test images): (990, 224, 224, 3)\n",
            "Shape of Y (test labels): (990, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model(num_conv_layers):\n",
        "  \"\"\"\n",
        "  Creates a CNN model with the specified number of convolutional layers.\n",
        "\n",
        "  Args:\n",
        "      num_conv_layers: The number of convolutional layers to use.\n",
        "\n",
        "  Returns:\n",
        "      A compiled TensorFlow Keras model.\n",
        "  \"\"\"\n",
        "  model = Sequential()\n",
        "\n",
        "  # First convolutional layer\n",
        "  model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(img_width, img_height, 3)))\n",
        "  model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "  # Additional convolutional layers (if specified)\n",
        "  for _ in range(1, num_conv_layers):\n",
        "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "  # Flatten the output\n",
        "  model.add(Flatten())\n",
        "\n",
        "  # Fully connected layers\n",
        "  model.add(Dense(64, activation='relu'))\n",
        "  model.add(Dense(1, activation='sigmoid'))  # Change to 'softmax' for multiple classes\n",
        "\n",
        "  # Compile the model\n",
        "  model.compile(loss='binary_crossentropy', optimizer=SGD(learning_rate=0.01), metrics=['accuracy'])\n",
        "\n",
        "  return model\n",
        "\n",
        "# Train models with different architectures (1 to 4 convolutional layers)\n",
        "for num_conv_layers in range(1, 5):\n",
        "  model = create_model(num_conv_layers)\n",
        "  model.fit(\n",
        "      train_generator,\n",
        "      steps_per_epoch=len(train_generator),\n",
        "      # epochs=50,\n",
        "      epochs=1,\n",
        "      validation_data=validation_generator,\n",
        "      validation_steps=len(validation_generator)\n",
        "  )\n",
        "\n",
        "  # Evaluate the model on the test set (optional)\n",
        "  test_loss, test_acc = model.evaluate(test_generator)\n",
        "  print(f\"Model with {num_conv_layers} convolutional layers: Test Accuracy = {test_acc:.4f}\")"
      ],
      "metadata": {
        "id": "SnKgS3cnr9Qz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "outputId": "035cee67-a01f-4256-e36d-1b92f5203eab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "38/70 [===============>..............] - ETA: 2:40 - loss: 0.6325 - accuracy: 0.7121"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-5abf741cbcd8>\u001b[0m in \u001b[0;36m<cell line: 35>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnum_conv_layers\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m   \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_conv_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m   model.fit(\n\u001b[0m\u001b[1;32m     38\u001b[0m       \u001b[0mtrain_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m       \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1805\u001b[0m                         ):\n\u001b[1;32m   1806\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1807\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1808\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1809\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 832\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    866\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 868\u001b[0;31m       return tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    869\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1321\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1322\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1325\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1486\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1487\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "At1bfsbEZMOY"
      },
      "source": [
        "Referensi yang harus dicoba:\n",
        "https://www.kaggle.com/code/banddaniel/brain-tumor-detection-w-keras-yolo-v8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dnvPpCLAlsLO"
      },
      "outputs": [],
      "source": [
        "# # Import necessary libraries\n",
        "\n",
        "# import os\n",
        "# import zipfile\n",
        "# import urllib.request\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# import numpy as np\n",
        "# import tensorflow as tf\n",
        "# from tensorflow.keras.utils import to_categorical\n",
        "# from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "# from tensorflow.keras.models import Sequential\n",
        "# from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "# from tensorflow.keras.preprocessing.image import load_img, img_to_array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8_yje0h49c79"
      },
      "outputs": [],
      "source": [
        "# # Download and extract the dataset\n",
        "# url = \"https://github.com/onesinus/datasets/raw/main/brain_tumor_detection.zip\"\n",
        "# filename = \"brain_tumor_detection.zip\"\n",
        "# urllib.request.urlretrieve(url, filename)\n",
        "# with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
        "#     zip_ref.extractall(\"dataset\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nFHnVe7qOTkG"
      },
      "outputs": [],
      "source": [
        "# def count_images_and_classes(data_dir):\n",
        "#     num_images = sum(len(files) for _, _, files in os.walk(os.path.join(data_dir, 'images')))\n",
        "#     unique_classes = set()\n",
        "#     label_dir = os.path.join(data_dir, 'labels')\n",
        "#     for label_file in os.listdir(label_dir):\n",
        "#         label_path = os.path.join(label_dir, label_file)\n",
        "#         with open(label_path, 'r') as f:\n",
        "#             labels = f.readlines()\n",
        "#         if labels:\n",
        "#             for label in labels:\n",
        "#                 class_label = int(label.split()[0])\n",
        "#                 unique_classes.add(class_label)\n",
        "#     num_classes = len(unique_classes)\n",
        "#     return num_images, num_classes\n",
        "\n",
        "# train_data_dir = 'dataset/Brain Tumor Detection/train'\n",
        "# test_data_dir = 'dataset/Brain Tumor Detection/test'\n",
        "# valid_data_dir = 'dataset/Brain Tumor Detection/valid'\n",
        "\n",
        "# train_image_count, train_num_classes = count_images_and_classes(train_data_dir)\n",
        "# test_image_count, test_num_classes = count_images_and_classes(test_data_dir)\n",
        "# valid_image_count, valid_num_classes = count_images_and_classes(valid_data_dir)\n",
        "\n",
        "# print(f'Found {train_image_count} images belonging to {train_num_classes} classes in the training directory.')\n",
        "# print(f'Found {test_image_count} images belonging to {test_num_classes} classes in the test directory.')\n",
        "# print(f'Found {valid_image_count} images belonging to {valid_num_classes} classes in the validation directory.')\n",
        "\n",
        "# output\n",
        "print(\"Found 6930 images belonging to 3 classes in the training directory.\")\n",
        "print(\"Found 990 images belonging to 3 classes in the test directory.\")\n",
        "print(\"Found 1980 images belonging to 3 classes in the validation directory.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l557M3-P9lhV"
      },
      "outputs": [],
      "source": [
        "# from keras.utils import to_categorical\n",
        "\n",
        "# def custom_data_generator(data_dir, batch_size=32, target_size=(150, 150), max_objects=3, num_classes=3):\n",
        "#     while True:\n",
        "#         # Get list of image files\n",
        "#         image_files = os.listdir(os.path.join(data_dir, 'images'))\n",
        "#         # np.random.shuffle(image_files)\n",
        "\n",
        "#         # Iterate over batches\n",
        "#         for i in range(0, len(image_files), batch_size):\n",
        "#             batch_image_files = image_files[i:i+batch_size]\n",
        "#             batch_images = []\n",
        "#             batch_labels = []\n",
        "\n",
        "#             # Load images and corresponding labels\n",
        "#             for image_file in batch_image_files:\n",
        "#                 # Load image\n",
        "#                 image_path = os.path.join(data_dir, 'images', image_file)\n",
        "#                 image = load_img(image_path, target_size=target_size)\n",
        "#                 image_array = img_to_array(image) / 255.0  # Normalize pixel values\n",
        "#                 batch_images.append(image_array)\n",
        "\n",
        "#                 # Load label\n",
        "#                 label_file = image_file.replace('.jpg', '.txt')\n",
        "#                 label_path = os.path.join(data_dir, 'labels', label_file)\n",
        "#                 with open(label_path, 'r') as f:\n",
        "#                     label_lines = f.readlines()\n",
        "\n",
        "#                 if label_lines:  # Check if label file is not empty\n",
        "#                     labels = []\n",
        "#                     for line in label_lines:\n",
        "#                         label_data = line.strip().split(' ')\n",
        "#                         # Update labels to reflect multiple classes\n",
        "#                         class_id = int(label_data[0])  # Extract class ID\n",
        "#                         labels.append(class_id)\n",
        "\n",
        "#                     # Ensure labels are within the range of num_classes\n",
        "#                     labels = [min(class_id, num_classes - 1) for class_id in labels]\n",
        "\n",
        "#                     # Perform one-hot encoding\n",
        "#                     one_hot_labels = to_categorical(labels, num_classes=num_classes)\n",
        "\n",
        "#                     # Convert one-hot encoded labels to one-dimensional array\n",
        "#                     one_dim_labels = np.argmax(one_hot_labels, axis=1)\n",
        "\n",
        "#                     # Pad labels with zeros to ensure fixed length\n",
        "#                     pad_length = max_objects - len(one_dim_labels)\n",
        "#                     one_dim_labels = np.pad(one_dim_labels, (0, pad_length), mode='constant')\n",
        "#                 else:\n",
        "#                     # If label file is empty, add placeholders\n",
        "#                     one_dim_labels = np.zeros((max_objects,), dtype=int)\n",
        "\n",
        "#                 batch_labels.append(one_dim_labels)\n",
        "\n",
        "#             yield np.array(batch_images), np.array(batch_labels)\n",
        "\n",
        "# train_data_dir = 'dataset/Brain Tumor Detection/train'\n",
        "# train_generator = custom_data_generator(train_data_dir, batch_size=100, target_size=(150, 150), num_classes=3)\n",
        "\n",
        "# test_data_dir = 'dataset/Brain Tumor Detection/test'\n",
        "# test_generator = custom_data_generator(test_data_dir, batch_size=100, target_size=(150, 150), num_classes=3)\n",
        "\n",
        "# valid_data_dir = 'dataset/Brain Tumor Detection/valid'\n",
        "# valid_generator = custom_data_generator(valid_data_dir, batch_size=100, target_size=(150, 150), num_classes=3)\n",
        "\n",
        "# # Iterate over batches and print labels\n",
        "# for i, (x_batch, y_batch) in enumerate(train_generator):\n",
        "#     # Display the first image in the batch\n",
        "#     plt.imshow(x_batch[0])\n",
        "#     plt.axis('off')  # Turn off axis labels\n",
        "#     plt.show()\n",
        "\n",
        "#     print(f\"Batch {i + 1} - Labels:\")\n",
        "#     print(y_batch[0])\n",
        "\n",
        "#     if i == 5:  # Print labels for the first 5 batches for demonstration\n",
        "#         break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SyAw1ZRS4rGU"
      },
      "outputs": [],
      "source": [
        "# # Define CNN models\n",
        "# models_configurations = [\n",
        "#     # Configuration a\n",
        "#     [\n",
        "#         Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n",
        "#         MaxPooling2D((2, 2)),\n",
        "#         Flatten(),\n",
        "#         Dense(128, activation='relu'),\n",
        "#         Dense(3, activation='softmax')\n",
        "#     ],\n",
        "#     # Configuration b\n",
        "#     [\n",
        "#         Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n",
        "#         Conv2D(64, (3, 3), activation='relu'),\n",
        "#         MaxPooling2D((2, 2)),\n",
        "#         Flatten(),\n",
        "#         Dense(128, activation='relu'),\n",
        "#         Dense(3, activation='softmax')\n",
        "#     ],\n",
        "#     # Configuration c\n",
        "#     [\n",
        "#         Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n",
        "#         Conv2D(64, (3, 3), activation='relu'),\n",
        "#         MaxPooling2D((2, 2)),\n",
        "#         Conv2D(128, (3, 3), activation='relu'),\n",
        "#         MaxPooling2D((2, 2)),\n",
        "#         Flatten(),\n",
        "#         Dense(128, activation='relu'),\n",
        "#         Dense(3, activation='softmax')\n",
        "#     ],\n",
        "#     # Configuration d\n",
        "#     [\n",
        "#         Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n",
        "#         Conv2D(64, (3, 3), activation='relu'),\n",
        "#         MaxPooling2D((2, 2)),\n",
        "#         Conv2D(128, (3, 3), activation='relu'),\n",
        "#         Conv2D(256, (3, 3), activation='relu'),\n",
        "#         MaxPooling2D((2, 2)),\n",
        "#         Flatten(),\n",
        "#         Dense(128, activation='relu'),\n",
        "#         Dense(3, activation='softmax')\n",
        "#     ]\n",
        "# ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hivZHc4j33ns"
      },
      "outputs": [],
      "source": [
        "# # Iterate over each model configuration\n",
        "# for i, layers_config in enumerate(models_configurations):\n",
        "#     print(f\"Training Model {chr(97 + i)}:\")\n",
        "\n",
        "#     # Define model\n",
        "#     model = Sequential(layers_config)\n",
        "\n",
        "#     # Compile model\n",
        "#     model.compile(optimizer='SGD', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "#     # Train model using generator\n",
        "#     history = model.fit(train_generator, epochs=1, steps_per_epoch=100,\n",
        "#                         validation_data=valid_generator, validation_steps=50)\n",
        "\n",
        "#     # Evaluate model\n",
        "#     test_loss, test_acc = model.evaluate(test_generator, steps=50)\n",
        "#     print(f\"Test Accuracy for Model {chr(97 + i)}:\", test_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iqmYopC0haFT"
      },
      "outputs": [],
      "source": [
        "# # Iterate over each model configuration\n",
        "# for i, layers_config in enumerate(models_configurations):\n",
        "#     print(f\"Training Model {chr(97 + i)}:\")\n",
        "\n",
        "#     # Define model\n",
        "#     model = Sequential(layers_config)\n",
        "\n",
        "#     # Compile model\n",
        "#     model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "#     # Train model using generator\n",
        "#     history = model.fit(train_generator, epochs=5, steps_per_epoch=100,\n",
        "#                         validation_data=valid_generator, validation_steps=50)\n",
        "\n",
        "#     # Evaluate model\n",
        "#     test_loss, test_acc = model.evaluate(test_generator, steps=50)\n",
        "#     print(f\"Test Accuracy for Model {chr(97 + i)}:\", test_acc)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}