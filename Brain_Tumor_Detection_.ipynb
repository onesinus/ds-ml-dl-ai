{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### zzzz"
      ],
      "metadata": {
        "id": "4PkfKuWmrg7Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import urllib.request\n",
        "import zipfile\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from tensorflow.keras.optimizers import SGD"
      ],
      "metadata": {
        "id": "3zeAJajdritU"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download and extract the dataset (assuming it's a zip file)\n",
        "url = \"https://github.com/onesinus/datasets/raw/main/brain_tumor_detection.zip\"\n",
        "filename = \"brain_tumor_detection.zip\"\n",
        "dataset_dir = \"/content/dataset\"  # Directory to store extracted data\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "os.makedirs(dataset_dir, exist_ok=True)\n",
        "\n",
        "if not os.path.exists(filename):\n",
        "  print(\"Downloading dataset...\")\n",
        "  urllib.request.urlretrieve(url, filename)\n",
        "  print(\"Download complete.\")\n",
        "\n",
        "# Extract the downloaded zip file\n",
        "if os.path.exists(filename):\n",
        "  print(\"Extracting dataset...\")\n",
        "  with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
        "    zip_ref.extractall(dataset_dir)\n",
        "  os.remove(filename)  # Remove downloaded zip\n",
        "  print(\"Extraction complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CSADOvx3txl9",
        "outputId": "ab05a9f1-5b80-4556-ae9a-0dc9d16cf444"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading dataset...\n",
            "Download complete.\n",
            "Extracting dataset...\n",
            "Extraction complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define label mapping (assuming class labels are encoded as strings)\n",
        "label_map = {'0': 0, '1': 1, '2': 2}  # Modify if class labels are encoded differently\n",
        "\n",
        "# Image dimensions (adjust if images have different sizes)\n",
        "img_width, img_height = 139, 132  # Example size, adjust based on your data\n",
        "\n",
        "# Data augmentation (optional, experiment to see if it improves performance)\n",
        "train_datagen = ImageDataGenerator(rescale=1./255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True)\n",
        "valid_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)"
      ],
      "metadata": {
        "id": "-nn9_rKe5-z8"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time # will be deleted later, just to save memory hehe\n",
        "\n",
        "def load_data(data_dir, images_dir_name, labels_dir_name, label_map):\n",
        "  \"\"\"Loads images and labels from separate directories, handling multiple labels per file.\"\"\"\n",
        "  images = []\n",
        "  labels = []\n",
        "  for filename in os.listdir(os.path.join(data_dir, images_dir_name)):\n",
        "    if filename.endswith(\".jpg\"):\n",
        "      img_path = os.path.join(data_dir, images_dir_name, filename)\n",
        "      img = tf.keras.preprocessing.image.load_img(img_path, target_size=(img_width, img_height))\n",
        "      img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
        "      img_array = img_array / 255.0  # Normalize pixel values\n",
        "\n",
        "      label_filename = os.path.splitext(filename)[0] + \".txt\"  # Remove extension and add .txt\n",
        "      label_path = os.path.join(data_dir, labels_dir_name, label_filename)\n",
        "\n",
        "      with open(label_path, 'r') as f:\n",
        "        label_lines = f.readlines()  # Read all lines in the label file\n",
        "\n",
        "      # Assuming multiple labels per line (modify if different)\n",
        "      one_hot_labels = np.zeros(len(label_map))  # Initialize one-hot encoded labels\n",
        "      for line in label_lines:\n",
        "        label_str = line.strip()\n",
        "        label_values = label_str.split()\n",
        "        class_value = label_values[0]\n",
        "        # print(label_str)\n",
        "        if class_value in label_map:\n",
        "          label_index = label_map[class_value]\n",
        "          one_hot_labels[label_index] = 1.0  # Set corresponding label to 1 in one-hot vector\n",
        "\n",
        "      images.append(img_array)\n",
        "      labels.append(one_hot_labels)\n",
        "\n",
        "  return np.array(images), np.array(labels)\n",
        "\n",
        "# Load training and validation data (assuming separate directories)\n",
        "train_images, train_labels = load_data(dataset_dir, 'Brain Tumor Detection/train/images', 'Brain Tumor Detection/train/labels', label_map)\n",
        "validation_images, validation_labels = load_data(dataset_dir, 'Brain Tumor Detection/valid/images', 'Brain Tumor Detection/valid/labels', label_map)\n",
        "test_images, test_labels = load_data(dataset_dir, 'Brain Tumor Detection/test/images', 'Brain Tumor Detection/test/labels', label_map)\n",
        "\n",
        "# Create training and validation generators\n",
        "train_generator = train_datagen.flow(train_images, train_labels, batch_size=100)\n",
        "time.sleep(1)\n",
        "validation_generator = valid_datagen.flow(validation_images, validation_labels, batch_size=100)\n",
        "time.sleep(1)\n",
        "test_generator = test_datagen.flow(test_images, test_labels, batch_size=100)"
      ],
      "metadata": {
        "id": "VpTAc8KUsAT3"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print shapes for training data\n",
        "print(f\"Shape of X (training images): {train_images.shape}\")\n",
        "print(f\"Shape of Y (training labels): {train_labels.shape}\")\n",
        "\n",
        "# print(f\"Training Image [0]: {train_images[0]}\")\n",
        "print(f\"Training Label [0]: {train_labels[0]}\")\n",
        "\n",
        "for i in range(1,10):\n",
        "  print(f\"Training Label [{i}]: {train_labels[i]}\")\n",
        "\n",
        "# Print shapes for validation data\n",
        "print(f\"Shape of X (validation images): {validation_images.shape}\")\n",
        "print(f\"Shape of Y (validation labels): {validation_labels.shape}\")\n",
        "\n",
        "# Assuming you have loaded test data using the same approach\n",
        "print(f\"Shape of X (test images): {test_images.shape}\")\n",
        "print(f\"Shape of Y (test labels): {test_labels.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HUOZhCNkvBR0",
        "outputId": "e7819c1b-b0dd-4d01-a2f0-b66585628571"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X (training images): (6930, 139, 132, 3)\n",
            "Shape of Y (training labels): (6930, 3)\n",
            "Training Label [0]: [1. 1. 1.]\n",
            "Training Label [1]: [0. 1. 0.]\n",
            "Training Label [2]: [0. 1. 0.]\n",
            "Training Label [3]: [1. 1. 1.]\n",
            "Training Label [4]: [1. 1. 1.]\n",
            "Training Label [5]: [1. 1. 0.]\n",
            "Training Label [6]: [1. 1. 1.]\n",
            "Training Label [7]: [0. 1. 1.]\n",
            "Training Label [8]: [1. 1. 1.]\n",
            "Training Label [9]: [0. 1. 0.]\n",
            "Shape of X (validation images): (1980, 139, 132, 3)\n",
            "Shape of Y (validation labels): (1980, 3)\n",
            "Shape of X (test images): (990, 139, 132, 3)\n",
            "Shape of Y (test labels): (990, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define CNN models\n",
        "models_configurations = [\n",
        "    # Configuration a\n",
        "    [\n",
        "        Conv2D(32, (3, 3), activation='relu', input_shape=(img_width, img_height, 3)),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Flatten(),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ],\n",
        "    # # Configuration b\n",
        "    # [\n",
        "    #     Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n",
        "    #     Conv2D(64, (3, 3), activation='relu'),\n",
        "    #     MaxPooling2D((2, 2)),\n",
        "    #     Flatten(),\n",
        "    #     Dense(128, activation='relu'),\n",
        "    #     Dense(3, activation='softmax')\n",
        "    # ],\n",
        "    # # Configuration c\n",
        "    # [\n",
        "    #     Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n",
        "    #     Conv2D(64, (3, 3), activation='relu'),\n",
        "    #     MaxPooling2D((2, 2)),\n",
        "    #     Conv2D(128, (3, 3), activation='relu'),\n",
        "    #     MaxPooling2D((2, 2)),\n",
        "    #     Flatten(),\n",
        "    #     Dense(128, activation='relu'),\n",
        "    #     Dense(3, activation='softmax')\n",
        "    # ],\n",
        "    # # Configuration d\n",
        "    # [\n",
        "    #     Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n",
        "    #     Conv2D(64, (3, 3), activation='relu'),\n",
        "    #     MaxPooling2D((2, 2)),\n",
        "    #     Conv2D(128, (3, 3), activation='relu'),\n",
        "    #     Conv2D(256, (3, 3), activation='relu'),\n",
        "    #     MaxPooling2D((2, 2)),\n",
        "    #     Flatten(),\n",
        "    #     Dense(128, activation='relu'),\n",
        "    #     Dense(3, activation='softmax')\n",
        "    # ]\n",
        "]"
      ],
      "metadata": {
        "id": "81ztowBVtZRb"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model(layers_config):\n",
        "  model = Sequential()\n",
        "  for layer_config in layers_config:\n",
        "    model.add(layer_config)\n",
        "\n",
        "  model.compile(loss='binary_crossentropy', optimizer='SGD', metrics=['accuracy'])\n",
        "  # model.compile(loss='binary_crossentropy', optimizer=SGD(learning_rate=0.01), metrics=['accuracy'])\n",
        "  # model.compile(optimizer='SGD', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "  return model\n",
        "\n",
        "# Iterate over each model configuration\n",
        "for i, layers_config in enumerate(models_configurations):\n",
        "    print(f\"Training Model {chr(97 + i)}:\")\n",
        "    model = create_model(layers_config)\n",
        "    model.fit(\n",
        "        train_generator,\n",
        "        steps_per_epoch=len(train_generator),\n",
        "        # epochs=50,\n",
        "        epochs=1,\n",
        "        validation_data=validation_generator,\n",
        "        validation_steps=len(validation_generator)\n",
        "    )\n",
        "\n",
        "    # # Evaluate the model on the test set (optional)\n",
        "    # test_loss, test_acc = model.evaluate(test_generator)\n",
        "    # print(f\"Model with {layers_config} convolutional layers: Test Accuracy = {test_acc:.4f}\")"
      ],
      "metadata": {
        "id": "SnKgS3cnr9Qz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f78e08c-f54f-4ea0-f237-20ff63dd1c80"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Model a:\n",
            "70/70 [==============================] - 132s 2s/step - loss: 0.6224 - accuracy: 0.7169 - val_loss: 0.5762 - val_accuracy: 0.7374\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "At1bfsbEZMOY"
      },
      "source": [
        "Referensi yang harus dicoba:\n",
        "https://www.kaggle.com/code/banddaniel/brain-tumor-detection-w-keras-yolo-v8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dnvPpCLAlsLO"
      },
      "outputs": [],
      "source": [
        "# # Import necessary libraries\n",
        "\n",
        "# import os\n",
        "# import zipfile\n",
        "# import urllib.request\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# import numpy as np\n",
        "# import tensorflow as tf\n",
        "# from tensorflow.keras.utils import to_categorical\n",
        "# from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "# from tensorflow.keras.models import Sequential\n",
        "# from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "# from tensorflow.keras.preprocessing.image import load_img, img_to_array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8_yje0h49c79"
      },
      "outputs": [],
      "source": [
        "# # Download and extract the dataset\n",
        "# url = \"https://github.com/onesinus/datasets/raw/main/brain_tumor_detection.zip\"\n",
        "# filename = \"brain_tumor_detection.zip\"\n",
        "# urllib.request.urlretrieve(url, filename)\n",
        "# with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
        "#     zip_ref.extractall(\"dataset\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nFHnVe7qOTkG"
      },
      "outputs": [],
      "source": [
        "# def count_images_and_classes(data_dir):\n",
        "#     num_images = sum(len(files) for _, _, files in os.walk(os.path.join(data_dir, 'images')))\n",
        "#     unique_classes = set()\n",
        "#     label_dir = os.path.join(data_dir, 'labels')\n",
        "#     for label_file in os.listdir(label_dir):\n",
        "#         label_path = os.path.join(label_dir, label_file)\n",
        "#         with open(label_path, 'r') as f:\n",
        "#             labels = f.readlines()\n",
        "#         if labels:\n",
        "#             for label in labels:\n",
        "#                 class_label = int(label.split()[0])\n",
        "#                 unique_classes.add(class_label)\n",
        "#     num_classes = len(unique_classes)\n",
        "#     return num_images, num_classes\n",
        "\n",
        "# train_data_dir = 'dataset/Brain Tumor Detection/train'\n",
        "# test_data_dir = 'dataset/Brain Tumor Detection/test'\n",
        "# valid_data_dir = 'dataset/Brain Tumor Detection/valid'\n",
        "\n",
        "# train_image_count, train_num_classes = count_images_and_classes(train_data_dir)\n",
        "# test_image_count, test_num_classes = count_images_and_classes(test_data_dir)\n",
        "# valid_image_count, valid_num_classes = count_images_and_classes(valid_data_dir)\n",
        "\n",
        "# print(f'Found {train_image_count} images belonging to {train_num_classes} classes in the training directory.')\n",
        "# print(f'Found {test_image_count} images belonging to {test_num_classes} classes in the test directory.')\n",
        "# print(f'Found {valid_image_count} images belonging to {valid_num_classes} classes in the validation directory.')\n",
        "\n",
        "# output\n",
        "print(\"Found 6930 images belonging to 3 classes in the training directory.\")\n",
        "print(\"Found 990 images belonging to 3 classes in the test directory.\")\n",
        "print(\"Found 1980 images belonging to 3 classes in the validation directory.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l557M3-P9lhV"
      },
      "outputs": [],
      "source": [
        "# from keras.utils import to_categorical\n",
        "\n",
        "# def custom_data_generator(data_dir, batch_size=32, target_size=(150, 150), max_objects=3, num_classes=3):\n",
        "#     while True:\n",
        "#         # Get list of image files\n",
        "#         image_files = os.listdir(os.path.join(data_dir, 'images'))\n",
        "#         # np.random.shuffle(image_files)\n",
        "\n",
        "#         # Iterate over batches\n",
        "#         for i in range(0, len(image_files), batch_size):\n",
        "#             batch_image_files = image_files[i:i+batch_size]\n",
        "#             batch_images = []\n",
        "#             batch_labels = []\n",
        "\n",
        "#             # Load images and corresponding labels\n",
        "#             for image_file in batch_image_files:\n",
        "#                 # Load image\n",
        "#                 image_path = os.path.join(data_dir, 'images', image_file)\n",
        "#                 image = load_img(image_path, target_size=target_size)\n",
        "#                 image_array = img_to_array(image) / 255.0  # Normalize pixel values\n",
        "#                 batch_images.append(image_array)\n",
        "\n",
        "#                 # Load label\n",
        "#                 label_file = image_file.replace('.jpg', '.txt')\n",
        "#                 label_path = os.path.join(data_dir, 'labels', label_file)\n",
        "#                 with open(label_path, 'r') as f:\n",
        "#                     label_lines = f.readlines()\n",
        "\n",
        "#                 if label_lines:  # Check if label file is not empty\n",
        "#                     labels = []\n",
        "#                     for line in label_lines:\n",
        "#                         label_data = line.strip().split(' ')\n",
        "#                         # Update labels to reflect multiple classes\n",
        "#                         class_id = int(label_data[0])  # Extract class ID\n",
        "#                         labels.append(class_id)\n",
        "\n",
        "#                     # Ensure labels are within the range of num_classes\n",
        "#                     labels = [min(class_id, num_classes - 1) for class_id in labels]\n",
        "\n",
        "#                     # Perform one-hot encoding\n",
        "#                     one_hot_labels = to_categorical(labels, num_classes=num_classes)\n",
        "\n",
        "#                     # Convert one-hot encoded labels to one-dimensional array\n",
        "#                     one_dim_labels = np.argmax(one_hot_labels, axis=1)\n",
        "\n",
        "#                     # Pad labels with zeros to ensure fixed length\n",
        "#                     pad_length = max_objects - len(one_dim_labels)\n",
        "#                     one_dim_labels = np.pad(one_dim_labels, (0, pad_length), mode='constant')\n",
        "#                 else:\n",
        "#                     # If label file is empty, add placeholders\n",
        "#                     one_dim_labels = np.zeros((max_objects,), dtype=int)\n",
        "\n",
        "#                 batch_labels.append(one_dim_labels)\n",
        "\n",
        "#             yield np.array(batch_images), np.array(batch_labels)\n",
        "\n",
        "# train_data_dir = 'dataset/Brain Tumor Detection/train'\n",
        "# train_generator = custom_data_generator(train_data_dir, batch_size=100, target_size=(150, 150), num_classes=3)\n",
        "\n",
        "# test_data_dir = 'dataset/Brain Tumor Detection/test'\n",
        "# test_generator = custom_data_generator(test_data_dir, batch_size=100, target_size=(150, 150), num_classes=3)\n",
        "\n",
        "# valid_data_dir = 'dataset/Brain Tumor Detection/valid'\n",
        "# valid_generator = custom_data_generator(valid_data_dir, batch_size=100, target_size=(150, 150), num_classes=3)\n",
        "\n",
        "# # Iterate over batches and print labels\n",
        "# for i, (x_batch, y_batch) in enumerate(train_generator):\n",
        "#     # Display the first image in the batch\n",
        "#     plt.imshow(x_batch[0])\n",
        "#     plt.axis('off')  # Turn off axis labels\n",
        "#     plt.show()\n",
        "\n",
        "#     print(f\"Batch {i + 1} - Labels:\")\n",
        "#     print(y_batch[0])\n",
        "\n",
        "#     if i == 5:  # Print labels for the first 5 batches for demonstration\n",
        "#         break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SyAw1ZRS4rGU"
      },
      "outputs": [],
      "source": [
        "# # Define CNN models\n",
        "# models_configurations = [\n",
        "#     # Configuration a\n",
        "#     [\n",
        "#         Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n",
        "#         MaxPooling2D((2, 2)),\n",
        "#         Flatten(),\n",
        "#         Dense(128, activation='relu'),\n",
        "#         Dense(3, activation='softmax')\n",
        "#     ],\n",
        "#     # Configuration b\n",
        "#     [\n",
        "#         Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n",
        "#         Conv2D(64, (3, 3), activation='relu'),\n",
        "#         MaxPooling2D((2, 2)),\n",
        "#         Flatten(),\n",
        "#         Dense(128, activation='relu'),\n",
        "#         Dense(3, activation='softmax')\n",
        "#     ],\n",
        "#     # Configuration c\n",
        "#     [\n",
        "#         Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n",
        "#         Conv2D(64, (3, 3), activation='relu'),\n",
        "#         MaxPooling2D((2, 2)),\n",
        "#         Conv2D(128, (3, 3), activation='relu'),\n",
        "#         MaxPooling2D((2, 2)),\n",
        "#         Flatten(),\n",
        "#         Dense(128, activation='relu'),\n",
        "#         Dense(3, activation='softmax')\n",
        "#     ],\n",
        "#     # Configuration d\n",
        "#     [\n",
        "#         Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n",
        "#         Conv2D(64, (3, 3), activation='relu'),\n",
        "#         MaxPooling2D((2, 2)),\n",
        "#         Conv2D(128, (3, 3), activation='relu'),\n",
        "#         Conv2D(256, (3, 3), activation='relu'),\n",
        "#         MaxPooling2D((2, 2)),\n",
        "#         Flatten(),\n",
        "#         Dense(128, activation='relu'),\n",
        "#         Dense(3, activation='softmax')\n",
        "#     ]\n",
        "# ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hivZHc4j33ns"
      },
      "outputs": [],
      "source": [
        "# # Iterate over each model configuration\n",
        "# for i, layers_config in enumerate(models_configurations):\n",
        "#     print(f\"Training Model {chr(97 + i)}:\")\n",
        "\n",
        "#     # Define model\n",
        "#     model = Sequential(layers_config)\n",
        "\n",
        "#     # Compile model\n",
        "#     model.compile(optimizer='SGD', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "#     # Train model using generator\n",
        "#     history = model.fit(train_generator, epochs=1, steps_per_epoch=100,\n",
        "#                         validation_data=valid_generator, validation_steps=50)\n",
        "\n",
        "#     # Evaluate model\n",
        "#     test_loss, test_acc = model.evaluate(test_generator, steps=50)\n",
        "#     print(f\"Test Accuracy for Model {chr(97 + i)}:\", test_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iqmYopC0haFT"
      },
      "outputs": [],
      "source": [
        "# # Iterate over each model configuration\n",
        "# for i, layers_config in enumerate(models_configurations):\n",
        "#     print(f\"Training Model {chr(97 + i)}:\")\n",
        "\n",
        "#     # Define model\n",
        "#     model = Sequential(layers_config)\n",
        "\n",
        "#     # Compile model\n",
        "#     model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "#     # Train model using generator\n",
        "#     history = model.fit(train_generator, epochs=5, steps_per_epoch=100,\n",
        "#                         validation_data=valid_generator, validation_steps=50)\n",
        "\n",
        "#     # Evaluate model\n",
        "#     test_loss, test_acc = model.evaluate(test_generator, steps=50)\n",
        "#     print(f\"Test Accuracy for Model {chr(97 + i)}:\", test_acc)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}